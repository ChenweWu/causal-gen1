{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "sys.path.append('../src/pgm')\n",
    "sys.path.append('../morphomnist')\n",
    "from typing import Dict, Any\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pgm.train_cf import cf_epoch\n",
    "from pgm.train_pgm import setup_dataloaders\n",
    "from pgm.flow_pgm import MorphoMNISTPGM\n",
    "\n",
    "class Hparams:\n",
    "    def update(self, dict):\n",
    "        for k, v in dict.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code needed for running evaluation should already be here:\n",
    "\n",
    "(i) https://github.com/biomedia-mira/causal-gen/blob/main/src/pgm/train_cf.py\n",
    "\n",
    "(ii) https://huggingface.co/spaces/mira-causality/counterfactuals/blob/main/app_utils.py\n",
    "\n",
    "I whipped up a more concise eval example below, hope it helps :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MorphoMNIST\n",
    "1. Load parent predictors - classifier/regressor for each parent in $\\mathbf{pa}_\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_path = '../checkpoints/t_i_d/sup_aux/checkpoint.pt'\n",
    "print(f'\\nLoading predictor checkpoint: {predictor_path}')\n",
    "predictor_checkpoint = torch.load(predictor_path)\n",
    "predictor_args = Hparams()\n",
    "predictor_args.update(predictor_checkpoint['hparams'])\n",
    "assert predictor_args.dataset == 'morphomnist'\n",
    "predictor = MorphoMNISTPGM(predictor_args).cuda()\n",
    "predictor.load_state_dict(predictor_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load PGM - Flow-based causal mechanisms for each node in the causal graph (except for $\\mathbf{x}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_path = '../checkpoints/t_i_d/sup_pgm/checkpoint.pt'\n",
    "print(f'\\nLoading PGM checkpoint: {pgm_path}')\n",
    "pgm_checkpoint = torch.load(pgm_path)\n",
    "pgm_args = Hparams()\n",
    "pgm_args.update(pgm_checkpoint['hparams'])\n",
    "assert pgm_args.dataset == 'morphomnist'\n",
    "pgm = MorphoMNISTPGM(pgm_args).cuda()\n",
    "pgm.load_state_dict(pgm_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load HVAE - causal mechanism for the image $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vae(vae_path):\n",
    "    print(f'\\nLoading VAE checkpoint: {vae_path}')\n",
    "    vae_checkpoint = torch.load(vae_path)\n",
    "    vae_args = Hparams()\n",
    "    vae_args.update(vae_checkpoint['hparams'])\n",
    "    vae_args.data_dir = 'your dataset here'\n",
    "\n",
    "    # init model\n",
    "    assert vae_args.hps == 'morphomnist'\n",
    "    if not hasattr(vae_args, 'vae'):\n",
    "        vae_args.vae = 'simple'\n",
    "\n",
    "    if vae_args.vae == 'hierarchical':\n",
    "        from vae import HVAE\n",
    "        vae = HVAE(vae_args).cuda()\n",
    "    elif vae_args.vae == 'simple':\n",
    "        from simple_vae import VAE\n",
    "        vae = VAE(vae_args).cuda()\n",
    "    else:\n",
    "        NotImplementedError\n",
    "    vae.load_state_dict(vae_checkpoint['ema_model_state_dict'])\n",
    "    return vae, vae_args\n",
    "\n",
    "model_name = ''\n",
    "vae_path = '../checkpoints/t_i_d/'+model_name+'/checkpoint.pt'\n",
    "vae, vae_args = load_vae(vae_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Counterfactual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphomnist.morpho import ImageMorphology\n",
    "# Refer to https://github.com/dccastro/Morpho-MNIST for details on Morpho-MNIST\n",
    "\n",
    "def get_intensity(x, threshold=0.5):\n",
    "    x = x.detach().cpu().numpy()[:, 0]\n",
    "    x_min, x_max = x.min(axis=(1, 2), keepdims=True), x.max(axis=(1, 2), keepdims=True)\n",
    "    mask = (x >= x_min + (x_max - x_min) * threshold)\n",
    "    return np.array([np.median(i[m]) for i, m in zip(x, mask)])\n",
    "\n",
    "def img_thickness(img, threshold, scale):\n",
    "    return ImageMorphology(np.asarray(img), threshold, scale).mean_thickness\n",
    "\n",
    "def unpack(args):\n",
    "    return img_thickness(*args)\n",
    "\n",
    "def get_thickness(x, threshold=0.5, scale=4, pool=None, chunksize=100):\n",
    "    imgs = x.detach().cpu().numpy()[:, 0]\n",
    "    args = ((img, threshold, scale) for img in imgs)\n",
    "    if pool is None:\n",
    "        gen = map(unpack, args)\n",
    "    else:\n",
    "        gen = pool.imap(unpack, args, chunksize=chunksize)\n",
    "    results = tqdm(gen, total=len(imgs), unit='img', ascii=True)\n",
    "    return list(results)\n",
    "\n",
    "def eval_cf_loop(\n",
    "    vae: nn.Module,\n",
    "    pgm: nn.Module,\n",
    "    predictor: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    file: Any,\n",
    "    te_cf: bool = False,\n",
    "    rand_sample: bool = False,\n",
    "    n_seeds: int = 3\n",
    "):\n",
    "    for do_pa in ['thickness', 'intensity', 'digit', None]:  # \"None\" is for random interventions\n",
    "        acc_runs = []\n",
    "        mae_runs = {\n",
    "            'thickness': {'predicted': [], 'measured': []},\n",
    "            'intensity': {'predicted': [], 'measured': []}\n",
    "        }\n",
    "\n",
    "        for seed in range(n_seeds):\n",
    "            print(f'do({(do_pa if do_pa is not None else \"random\")}):')\n",
    "            targets, preds, x_cfs = cf_epoch(vae, pgm, predictor, dataloaders, do_pa=do_pa, te_cf=te_cf, rand_sample=rand_sample)\n",
    "            acc = (targets['digit'].argmax(-1).numpy() == preds['digit'].argmax(-1).numpy()).sum() / 10000\n",
    "            print(f'predicted digit acc:', acc)\n",
    "\n",
    "            measured = {}\n",
    "            measured['intensity'] = torch.tensor(get_intensity((x_cfs + 1.0) * 127.5))\n",
    "            with multiprocessing.Pool() as pool:\n",
    "                measured['thickness'] = torch.tensor(get_thickness((x_cfs + 1.0) * 127.5, pool=pool, chunksize=250))\n",
    "\n",
    "            mae = {'thickness': {}, 'intensity': {}}\n",
    "            for k in ['thickness', 'intensity']:\n",
    "                min_max = dataloaders['train'].dataset.min_max[k]\n",
    "                _min, _max = min_max[0], min_max[1]\n",
    "                preds_k = ((preds[k] + 1) / 2) * (_max - _min) + _min\n",
    "                targets_k = ((targets[k] + 1) / 2) * (_max - _min) + _min\n",
    "                mae[k]['predicted'] = (targets_k - preds_k).abs().mean().item()\n",
    "                mae[k]['measured'] = (targets_k - measured[k]).abs().mean().item()\n",
    "                print(f'predicted {k} mae:', mae[k]['predicted'])\n",
    "                print(f'measured {k} mae:', mae[k]['measured'])\n",
    "\n",
    "            acc_runs.append(acc)\n",
    "            for k in ['thickness', 'intensity']:\n",
    "                mae_runs[k]['predicted'].append(mae[k]['predicted'])\n",
    "                mae_runs[k]['measured'].append(mae[k]['measured'])\n",
    "\n",
    "            file.write(\n",
    "                f'\\ndo({(do_pa if do_pa is not None else \"random\")}) | digit acc: {acc}, ' +\n",
    "                f'thickness mae (predicted): {mae[\"thickness\"][\"predicted\"]}, ' +\n",
    "                f'thickness mae (measured): {mae[\"thickness\"][\"measured\"]}, ' +\n",
    "                f'intensity mae (predicted): {mae[\"intensity\"][\"predicted\"]}, ' +\n",
    "                f'intensity mae (measured): {mae[\"intensity\"][\"measured\"]} | seed {seed}'\n",
    "            )\n",
    "            file.flush()\n",
    "            gc.collect()\n",
    "\n",
    "        v = 'Used total effect counterfactuals: '+ str(te_cf)\n",
    "        file.write(\n",
    "            f'\\n{(v if vae.cond_prior else \"\")}\\n' +\n",
    "            f'digit acc | mean: {np.array(acc_runs).mean()} - std: {np.array(acc_runs).std()}\\n' +\n",
    "            f'thickness mae (predicted) | mean: {np.array(mae_runs[\"thickness\"][\"predicted\"]).mean()} - std: {np.array(mae_runs[\"thickness\"][\"predicted\"]).std()}\\n' +\n",
    "            f'thickness mae (measured) | mean: {np.array(mae_runs[\"thickness\"][\"measured\"]).mean()} - std: {np.array(mae_runs[\"thickness\"][\"measured\"]).std()}\\n' +\n",
    "            f'intensity mae (predicted) | mean: {np.array(mae_runs[\"intensity\"][\"predicted\"]).mean()} - std: {np.array(mae_runs[\"intensity\"][\"predicted\"]).std()}\\n' +\n",
    "            f'intensity mae (measured) | mean: {np.array(mae_runs[\"intensity\"][\"measured\"]).mean()} - std: {np.array(mae_runs[\"intensity\"][\"measured\"]).std()}\\n'\n",
    "        )\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "for model_name in [\n",
    "'add your model names here'\n",
    "]:\n",
    "    file = open(f'./morphomnist/eval_{model_name}.txt', 'a')\n",
    "    vae_path = '../checkpoints/t_i_d/'+model_name+'/checkpoint.pt'\n",
    "    vae, vae_args = load_vae(vae_path)\n",
    "\n",
    "    assert pgm_args.dataset == 'morphomnist'\n",
    "    pgm_args.bs = 32\n",
    "    dataloaders = setup_dataloaders(pgm_args)\n",
    "\n",
    "    if vae.cond_prior:  # Optional\n",
    "        # \"direct effect\" counterfactuals followed by \"total effect\" counterfactuals\n",
    "        for te_cf in [False, True]:\n",
    "            eval_cf_loop(vae, pgm, predictor, dataloaders, file, te_cf)\n",
    "    else:\n",
    "        eval_cf_loop(vae, pgm, predictor, dataloaders, file, rand_sample=True)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UKBB/MIMIC\n",
    "1. Load parent predictors, PGM and HVAE similar to above\n",
    "2. Run counterfactual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EMA\n",
    "from pgm.dscm import DSCM\n",
    "from pgm.layers import TraceStorage_ELBO\n",
    "\n",
    "# some compatibility stuff\n",
    "args = Hparams()\n",
    "args.beta = vae_args.beta\n",
    "args.parents_x = vae_args.parents_x\n",
    "args.input_res = vae_args.input_res\n",
    "args.grad_clip = vae_args.grad_clip\n",
    "args.grad_skip = vae_args.grad_skip\n",
    "args.elbo_constraint = 1.8412 # train set -elbo\n",
    "args.wd = vae_args.wd\n",
    "args.betas = vae_args.betas\n",
    "args.lmbda_init = 0\n",
    "args.damping = 100\n",
    "\n",
    "# init model\n",
    "if not hasattr(vae_args, 'dataset'):\n",
    "    args.dataset = 'ukbb'\n",
    "\n",
    "# Option 1: use base model (before counterfactual training)\n",
    "dscm = DSCM(args, pgm, predictor, vae)\n",
    "dscm.cuda();\n",
    "\n",
    "# Option 2: use counterfactual fine-tuned model\n",
    "dscm_path = os.path.join('../checkpoints', 'yourmodel', 'checkpoint.pt')\n",
    "print(f'\\nLoading DSCM checkpoint: {dscm_path}')\n",
    "dscm_checkpoint = torch.load(dscm_path)\n",
    "dscm_args = Hparams()\n",
    "dscm_args.update(dscm_checkpoint['hparams'])\n",
    "dscm_args.dataset = 'ukbb'\n",
    "dscm = DSCM(dscm_args, pgm, predictor, vae).cuda()\n",
    "dscm.load_state_dict(dscm_checkpoint['ema_model_state_dict'])\n",
    "dscm.cuda();\n",
    "\n",
    "# setup dataloaders\n",
    "pgm_args.bs = 8\n",
    "pgm_args.concat_pa = False\n",
    "dataloaders = setup_dataloaders(pgm_args)\n",
    "ema = EMA(dscm, beta=0.999)\n",
    "\n",
    "# set some defaults\n",
    "args.do_pa = None\n",
    "args.plot_freq = 1\n",
    "args.save_dir = './'\n",
    "args.step = 0\n",
    "args.imgs_plot = 10\n",
    "args.alpha = 1\n",
    "args.cf_particles = 1\n",
    "args.cond_prior = vae_args.cond_prior\n",
    "elbo_fn = TraceStorage_ELBO(num_particles=1)\n",
    "\n",
    "# run counterfactual evaluation\n",
    "with torch.no_grad():\n",
    "    copy_do_pa = copy.deepcopy(args.do_pa)\n",
    "    for pa_k in list(dscm.pgm.variables.keys()) + [None]:\n",
    "        args.do_pa = pa_k\n",
    "        valid_stats, metrics = cf_epoch(\n",
    "            args, dscm, ema, dataloaders, elbo_fn, None, split='valid'\n",
    "        )\n",
    "        print(f'valid do({pa_k}) | ' + ' - '.join(f'{k}: {v:.4f}' for k, v in valid_stats.items()))\n",
    "        print(f'valid do({pa_k}) | ' + ' - '.join(f'{k}: {v:.4f}' for k, v in metrics.items()))\n",
    "args.do_pa = copy_do_pa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
